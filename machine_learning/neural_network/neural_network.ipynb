{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:right\"> Antoine Moulin & Alberto Begu√©</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA 210\n",
    "# Computer Lab: Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lab's authors:</b> A. Garcia, A. Lambert, G. Staerman, G. Varni <br/>\n",
    "<b>Date:</b> April 6, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a neural network with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = np.array([11, 50, 62])\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in np.arange(sample_index.shape[0]):\n",
    "    plt.subplot(131 + i)\n",
    "    plt.imshow(digits.images[sample_index[i]], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "    plt.title(\"image label: %d\" % digits.target[sample_index[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.33)\n",
    "\n",
    "# normalize the training set and apply the transformation to the train set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scale = scaler.transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "# print a sample normalized\n",
    "plt.subplot(131)\n",
    "plt.imshow(X_train_scale[0].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % y_train[0])\n",
    "\n",
    "# print the original sample\n",
    "plt.subplot(132)\n",
    "original_sample = X_train_scale[0]*np.sqrt(scaler.var_) + scaler.mean_\n",
    "plt.imshow(original_sample.reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % y_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train, Y_test = to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is needed because in the end, we will have a vector of probabilities with components between $0$ and $1$ and then we take the argmax as the predicted class. Besides, the formula of the cross entropy supposes that the real output is $0$ or $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 0s 253us/step - loss: 2.3265 - acc: 0.1588\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 0s 29us/step - loss: 2.1396 - acc: 0.1945\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 0s 25us/step - loss: 2.0235 - acc: 0.2577\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 0s 26us/step - loss: 1.9146 - acc: 0.2868\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 0s 26us/step - loss: 1.8227 - acc: 0.3150\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 0s 26us/step - loss: 1.7592 - acc: 0.3948\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 0s 28us/step - loss: 1.6971 - acc: 0.4356\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 0s 26us/step - loss: 1.6443 - acc: 0.4647\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 0s 25us/step - loss: 1.5917 - acc: 0.5278\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 1.5444 - acc: 0.5769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24270fbd4a8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "n_i = X_train.shape[1]\n",
    "n_h = 5\n",
    "n_o = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(n_o))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3 </h3>\n",
    "\n",
    "Show that minimizing the cross entropy loss is equivalent to maximizing the log-likelihood of the observations under a well chosent probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let suppose that our observations follow a Bernoulli distribution. Then, for a probability $p$ obtained with our model, the probability that the output class is $y$ is given by\n",
    "\n",
    "$$\\mathbb{P} \\left( y | p \\right) = p^{y}(1 - p)^{1 - y}$$\n",
    "\n",
    "The likelihood is given by :\n",
    "\n",
    "$$\\mathbb{P} \\left( y | x \\right) = \\prod_{i=1}^{n} \\mathbb{P} \\left( y | x_{i} \\right)^{y_{i}} \\left( 1 - \\mathbb{P} \\left( y | x_{i} \\right) \\right)^{1 - y_{i}}$$\n",
    "\n",
    "And the log-likelihood :\n",
    "\n",
    "$$\\log \\mathbb{P} \\left( y | x \\right) = \\sum_{i=1}^{n} y_{i} \\log \\mathbb{P} \\left( y | x_{i} \\right) + \\left( 1 - y_{i} \\right) \\log \\left( 1 - \\mathbb{P} \\left( y | x_{i} \\right) \\right)$$\n",
    "\n",
    "Thus, we see that the cross entropy loss is given by $- \\log \\mathbb{P} \\left( y | x \\right)$. Then, minimizing the cross entropy loss is equivalent to maximizing the log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 4 </h3>\n",
    "\n",
    "Play with the size of the hidden layer. How does the optimization process behave at fixed learning rate and epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 0s 164us/step - loss: 2.4981 - acc: 0.1812\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 2.0298 - acc: 0.3392\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 0s 28us/step - loss: 1.7371 - acc: 0.4422\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 1.5233 - acc: 0.5428\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 1.3516 - acc: 0.6367\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 1.2214 - acc: 0.6949\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 0s 28us/step - loss: 1.1188 - acc: 0.7440\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 0s 27us/step - loss: 1.0306 - acc: 0.7739\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 0s 28us/step - loss: 0.9600 - acc: 0.8121\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 0s 33us/step - loss: 0.8961 - acc: 0.8213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x242750ca4a8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_i = X_train.shape[1]\n",
    "n_h = 20\n",
    "n_o = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(n_o))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is higher if we increase the number of hidden units. But it is useless to have too much hidden units if the problem is quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5 </h3>\n",
    "\n",
    "Explain the following sentence: \"Feed-forward neural networks with one hidden layer are known to be universal approximators\". What are the requirements on the activation function for this to be true? How does the width of such a network evolves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the universal approximation theorem, we can use a feed-forward network with a single hidden layer to approximate any continuous functions on compact subsets of $\\mathbb{R}^{n}$, where $n \\in \\mathbb{N}$. For this purpose, we need the activation function to be nonconstant, bounded and continuous.\n",
    "\n",
    "It has been shown recently that it actually works with the ReLU function and that the number of hidden units is limited by $n + 4$ if the input has a dimension $n$.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Universal_approximation_theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 6 </h3>\n",
    "\n",
    "Set $n_{h} = 100$. Play with the learning rate: how does the optimization process behave when the learning rate is too high/too low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 0s 314us/step - loss: 0.9695 - acc: 0.7124\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 0s 35us/step - loss: 0.2960 - acc: 0.9293\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 0s 32us/step - loss: 0.1648 - acc: 0.9726\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 0s 31us/step - loss: 0.1327 - acc: 0.9726\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 0s 32us/step - loss: 0.0896 - acc: 0.9867\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 0s 32us/step - loss: 0.0711 - acc: 0.9917\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 0s 31us/step - loss: 0.0604 - acc: 0.9909\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 0s 35us/step - loss: 0.0518 - acc: 0.9950\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 0s 30us/step - loss: 0.0421 - acc: 0.9967\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 0s 30us/step - loss: 0.0355 - acc: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2427af1ff28>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_i = X_train.shape[1]\n",
    "n_h = 100\n",
    "n_o = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(n_o))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.1),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the learning rate is too high, the precision is really low because the step is too large. However, the training is faster. When the learning rate is too low, the optimization does not give the minimum if the number of steps is not enough. Thus, the convergence is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 7 </h3>\n",
    "\n",
    "Test different optimization techniques (https://keras.io/optimizers/). What is the main difference between Adam and Adadelta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 1s 425us/step - loss: 1.7534 - acc: 0.4422\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 0s 34us/step - loss: 0.8067 - acc: 0.8337\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 0s 33us/step - loss: 0.5001 - acc: 0.9185\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 0s 32us/step - loss: 0.3518 - acc: 0.9485\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 0s 33us/step - loss: 0.2700 - acc: 0.9643\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 0s 32us/step - loss: 0.2161 - acc: 0.9692\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 0s 33us/step - loss: 0.1786 - acc: 0.9726\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 0s 36us/step - loss: 0.1523 - acc: 0.9800\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 0s 35us/step - loss: 0.1317 - acc: 0.9825\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 0s 35us/step - loss: 0.1131 - acc: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2427efd5898>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_i = X_train.shape[1]\n",
    "n_h = 100\n",
    "n_o = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(n_o))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a neural network with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by implementing the softmax function, sigmoid and its derivative, as well as negative log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    return None\n",
    "\n",
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    # TODO\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can complete the following python class to get your numpy network. This class is designed to be \"scikit like\", meaning that once it has been correctly completed, you can just execute \n",
    "\n",
    "-model = NeuralNet(n_features, n_hidden, n_classes); model.fit(X_train,Y_train,lr=0.1, n_epochs=20)\n",
    "\n",
    "to get it to work. Hints about how to fill the methods can be found in the pdf subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer and a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        return y\n",
    "    \n",
    "    def forward_with_hidden(self, X):\n",
    "        # TODO\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return L\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return grads\n",
    "\n",
    "    def train_sample(self, x, y, lr):\n",
    "        # TODO\n",
    "\n",
    "    def fit(self, X_train, Y_train , lr, n_epochs):\n",
    "        # TODO\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        return \n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
