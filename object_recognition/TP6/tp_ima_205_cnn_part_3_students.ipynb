{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHDUBjk341xU"
   },
   "source": [
    "# TP Convolutional Neural Networks in tensorflow and keras - part 3\n",
    "\n",
    "Author : Alasdair Newson\n",
    "\n",
    "alasdair.newson@telecom-paristech.fr\n",
    "\n",
    "In this session, we shall be looking at autoencoders. In particular, we shall apply these autoencoders to image denoising, on simple images from the MNIST dataset.\n",
    "\n",
    "First, let's load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1663,
     "status": "ok",
     "timestamp": 1556034592966,
     "user": {
      "displayName": "Antoine Moulin",
      "photoUrl": "",
      "userId": "06635915011281090521"
     },
     "user_tz": -120
    },
    "id": "y7iXI90q41xX",
    "outputId": "2626c9fe-a9c3-4bbd-9506-22d965aa2a92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2m_1z9Ks41xc"
   },
   "source": [
    "Now, we are going to create a simple autoencoder based on an MLP architecture, in Keras. The architecture is the following :\n",
    "\n",
    "- Encoder :\n",
    "    - Flatten input\n",
    "    - Dense layer, of output size $d$\n",
    "    - Leaky ReLU ($\\alpha$=0.2)\n",
    "- Decoder :\n",
    "    - Dense Layer, output size 784 (28$\\times$28)\n",
    "    - Sigmoid activation\n",
    "    - Reshape, to size $28\\times28$\n",
    "    \n",
    "The following code defines an autoencoder Class (using Python classes), which creates the autoencoder. Modify this class to implement the MLP autoencoder described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "567VLUHx41xe"
   },
   "outputs": [],
   "source": [
    "class autoencoder():\n",
    "    def __init__(self,dataset_name='mnist',architecture='mlp'):\n",
    "\n",
    "        X_train = self.load_data(dataset_name)\n",
    "        optimizer = 'adadelta'\n",
    "\n",
    "        # image parameters\n",
    "        self.epochs = 51\n",
    "        self.error_list = np.zeros((self.epochs,1))\n",
    "        self.img_rows = X_train.shape[1]\n",
    "        self.img_cols = X_train.shape[2]\n",
    "        self.img_channels = X_train.shape[3]\n",
    "        self.img_size = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
    "        self.z_dim = 32\n",
    "        self.sample_interval = 50\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.ae = self.build_ae()\n",
    "        self.ae.summary()\n",
    "        self.ae.compile(optimizer=optimizer, loss='binary_crossentropy') #binary cross-entropy loss, because mnist is grey-scale\n",
    "\n",
    "    def build_ae(self):\n",
    "\n",
    "        n_pixels = self.img_rows*self.img_cols*self.img_channels\n",
    "\n",
    "        # FULLY CONNECTED (MLP)\n",
    "\n",
    "        #BEGIN FILL IN CODE\n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        \n",
    "        encoder = Flatten()(input_img)\n",
    "        encoder = Dense(self.z_dim, activation=LeakyReLU(alpha=0.2))(encoder)\n",
    "        \n",
    "        decoder = Dense(n_pixels, activation='sigmoid')(encoder)\n",
    "        decoder = Reshape(self.img_shape)(decoder)\n",
    "        \n",
    "        ae_model = Model(input_img, decoder)\n",
    "        # END FILL IN CODE\n",
    "\n",
    "        #output the model\n",
    "        return ae_model\n",
    "    \n",
    "    \n",
    "    def load_data(self,dataset_name):\n",
    "        # Load the dataset\n",
    "        if(dataset_name == 'mnist'):\n",
    "            (X_train, _), (_, _) = mnist.load_data()\n",
    "        else:\n",
    "            print('Error, unknown database')\n",
    "\n",
    "        # normalise images between 0 and 1\n",
    "        X_train = X_train/255.0\n",
    "        #add a channel dimension, if need be (for mnist data)\n",
    "        if(X_train.ndim ==3):\n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "        return X_train\n",
    "\n",
    "    def test_images(self, test_imgs, image_filename):\n",
    "        # this function shows some input/output images for the autoencoder\n",
    "        n_images = test_imgs.shape[0]\n",
    "        #get output imagesq\n",
    "        output_imgs = self.ae.predict( test_imgs )\n",
    "\n",
    "        r = 2\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            #black and white images\n",
    "            axs[0,j].imshow(test_imgs[j, :, :, 0], cmap='gray')\n",
    "            axs[0,j].axis('off')\n",
    "            axs[1,j].imshow(output_imgs[j, :, :, 0], cmap='gray')\n",
    "            axs[1,j].axis('off')\n",
    "            fig.savefig(image_filename)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hMXU98R41xh"
   },
   "source": [
    "Now, modify the code in the next cell to train the autoencoder. \n",
    "\n",
    "In order to monitor the autoencoder's progression, use the ```test_images()``` function defined above to write the autoencoding output of some random images. You can do this every 'sample_interval' steps (a parameter of the autoencoder class. \n",
    "\n",
    "Note, in Keras, you can use the following function :\n",
    "\n",
    "- ```model.train_on_batch```($x,\\hat{y}$),  where $\\hat{y}$ is the target data (in the case of the autoencoder, this is the input data itself)\n",
    "\n",
    "to carry out a training step on a single batch, rather than the function model.fit(). This can be useful for seeing the evolution of the autoencoder's progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13173,
     "status": "ok",
     "timestamp": 1556043226020,
     "user": {
      "displayName": "Antoine Moulin",
      "photoUrl": "",
      "userId": "06635915011281090521"
     },
     "user_tz": -120
    },
    "id": "3UswsTsK41xi",
    "outputId": "7719c9c7-26ca-4273-bb6c-8df77c0ea148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 784)               25872     \n",
      "_________________________________________________________________\n",
      "reshape_13 (Reshape)         (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the output image directory\n",
    "if (os.path.isdir('images')==0):\n",
    "    os.mkdir('images')\n",
    "\n",
    "if (os.path.isdir('images/part3_without_noise')==0):\n",
    "    os.mkdir('images/part3_without_noise')\n",
    "    \n",
    "#choose dataset\n",
    "dataset_name = 'mnist'#\n",
    "batch_size=128\n",
    "\n",
    "#create AE model\n",
    "ae = autoencoder(dataset_name)\n",
    "#load dataset\n",
    "X_train = ae.load_data(ae.dataset_name)\n",
    "\n",
    "# Now, train model\n",
    "#BEGIN INSERT CODE\n",
    "number_steps = X_train.shape[0]//batch_size\n",
    "\n",
    "for epoch in range(ae.epochs):\n",
    "    np.random.shuffle(X_train)\n",
    "    \n",
    "    for i in range(number_steps):\n",
    "        batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "    \n",
    "        ae.ae.train_on_batch(batch_x, batch_x)\n",
    "    \n",
    "        if i%(2*ae.sample_interval) == 0 and epoch%(ae.sample_interval/5) == 0:\n",
    "            ae.test_images(X_train[i*batch_size: i*batch_size + 5], './images/part3_without_noise/epoch_{}_step_{}.jpg'.format(epoch, i*batch_size))\n",
    "# END INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwzdsJqt41xl"
   },
   "source": [
    "Now, we can train a denoising autoencoder. The denoising autoencoder minimises the following loss function :\n",
    "\n",
    "- $\\mathcal{L}(x) = || x - D \\circ E (x+\\eta)||^2_2$,\n",
    "\n",
    "where $\\eta$ is some noise with a fixed standard deviation.\n",
    "\n",
    "This is quite simple to implement using the above code. Instead of putting the real images as the input, just replace them with the real images, with noise added. Use Gaussian additive noise, with a relatively high standard deviation ($\\sigma=20.0/255.0$, for example).\n",
    "\n",
    "In the following cell, implement the training of the denoising autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BycDu3dk41xm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 784)               25872     \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the output image directory\n",
    "if (os.path.isdir('images')==0):\n",
    "    os.mkdir('images')\n",
    "\n",
    "if (os.path.isdir('images/part3_with_noise')==0):\n",
    "    os.mkdir('images/part3_with_noise')\n",
    "    \n",
    "#choose dataset\n",
    "dataset_name = 'mnist'#\n",
    "batch_size=128\n",
    "\n",
    "#create AE model\n",
    "ae = autoencoder(dataset_name)\n",
    "#load dataset\n",
    "X_train = ae.load_data(ae.dataset_name)\n",
    "noise = np.random.normal(0, 20./255., X_train.shape)\n",
    "\n",
    "# Now, train model\n",
    "#BEGIN INSERT CODE\n",
    "number_steps = X_train.shape[0]//batch_size\n",
    "\n",
    "for epoch in range(ae.epochs):\n",
    "    np.random.shuffle(X_train)\n",
    "    np.random.shuffle(noise)\n",
    "    for i in range(number_steps):\n",
    "        batch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        batch_noise = noise[i*batch_size:(i+1)*batch_size]\n",
    "    \n",
    "        ae.ae.train_on_batch(batch_x + batch_noise, batch_x)\n",
    "    \n",
    "        if i%(2*ae.sample_interval) == 0 and epoch%(ae.sample_interval/5) == 0:\n",
    "            ae.test_images(X_train[i*batch_size:i*batch_size+5] + noise[i*batch_size:i*batch_size+5], './images/part3_with_noise/epoch_{}_step_{}.jpg'.format(epoch, i*batch_size))\n",
    "# END INSERT CODE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp_ima_205_cnn_part_3_students.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
