{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">Antoine Moulin</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SD-TSIA211 </h1>\n",
    "<h1> Computer Lab: Nonnegative Matrix Factorization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.optimize import check_grad\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Database </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 1.1 </h3>\n",
    "\n",
    "Download and extract the database of faces, collected by AT&T Laboratories Cambridge,\n",
    "on https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.\n",
    "How many images are there in the database? How many pixels are there in each image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix_from_faces(folder='att_faces', minidata=False):\n",
    "    # load images\n",
    "    # 400 images of size (112, 92)\n",
    "    M = []\n",
    "    if minidata is True:\n",
    "        nb_subjects = 1\n",
    "    else:\n",
    "        nb_subjects = 40\n",
    "    for subject in range(1, nb_subjects + 1):\n",
    "        for image in range(1, 11):\n",
    "            face = plt.imread(folder + '/s' + str(subject)\n",
    "                              + '/' + str(image) + '.pgm')\n",
    "            M.append(face.ravel())\n",
    "\n",
    "    return np.array(M, dtype=float)\n",
    "\n",
    "full_db = build_matrix_from_faces()\n",
    "mini_db = build_matrix_from_faces(minidata = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_db = mini_db # just change this line to change the db used\n",
    "n, p = current_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 400 images of 92 x 112 pixels, i.e. 10304 pixels in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2 - Presentation of the model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $n$ images with $p$ pixels. We are going to vectorize the images, that is consider them as a mere list of $p$ pixel values. Then, we stack them all in a matrix $M$ of size $n \\times p$.\n",
    "\n",
    "The goal of Nonnegative Matrix Factorization (NMF) is to (approximately) factorize the matrix $M$, which has only nonnegative entries, into the product of two matrices $W$ and $H$ so that $M \\approx WH$. $W$ is a $n \\times k$ nonnegative matrix and $H$ is a $k \\times p$ nonnegative matrix, where $k \\leq min(n, p)$ is a parameter of the model called the dimension of the latent space. Each line $H_{j, :}$ of $H$ can be interpreted as the image of a piece of face. The coefficient $W_{i, j}$ tells us which proportion of image $H_{j, :}$ is present in the face $M_{i, :}$.\n",
    "\n",
    "The model can be trained by solving the optimization problem\n",
    "\n",
    "$$ \\left( \\widehat{W}, \\widehat{H} \\right) \\in \\arg \\min_{W \\geq 0, H \\geq 0} \\frac{1}{2np} \\sum_{i=1}^{n} \\sum_{l=1}^{p} \\left( M_{i, l} - \\sum_{j=1}^{k} W_{i, j} H_{j, l} \\right)^{2}$$\n",
    "\n",
    "$$\\left( \\widehat{W}, \\widehat{H} \\right) \\in \\arg \\min_{W \\geq 0, H \\geq 0} \\frac{1}{2np} \\left|\\left| M - WH \\right|\\right|_{F}^{2} $$\n",
    "\n",
    "where $\\left|\\left| \\cdot \\right|\\right|_{F}$ is Frobenius's norm.\n",
    "\n",
    "When $k = 1$ the solution is proportional to the left and right singular vectors of $M$ associated with the singular value with highest magnitude. In general, we need to use an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 2.1 </h3>\n",
    "\n",
    "Show that the objective function is not convex. Calculate its gradient. Is the gradient Lipschitz continuous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let define $g$ the objective function as: \n",
    "$$ g : \\begin{array} \\left \\mathbb{R}^{n \\times k} \\times \\mathbb{R}^{k \\times p} \\longrightarrow \\mathbb{R} \\\\\n",
    "(W, H) \\mapsto \\frac{1}{2np} || M - WH ||_{F}^{2} \\end{array}$$ and <b>show that $g$ is not convex</b>.\n",
    "\n",
    "In order to do this, let use the contraposed of the following property:\n",
    "\n",
    "Let $\\alpha \\in \\mathbb{N} \\setminus \\lbrace 0, 1 \\rbrace$. If $f : \\mathbb{R}^{\\alpha} \\longrightarrow \\mathbb{R}$ is convex, then for all $i, j \\in [\\![ 1, \\alpha ]\\!]$, $f_{i, j}$ is convex, where $$f_{i, j} : \\begin{array} \\left \\mathbb{R}^{2} \\longrightarrow \\mathbb{R} \\\\\n",
    "(x_{i}, x_{j}) \\mapsto f(0, ..., 0, x_{i}, 0, ..., 0, x_{j}, 0, ..., 0) \\end{array} $$\n",
    "\n",
    "\n",
    "Then, we just have to find a partial function of $g$ that is not convex in order to prove that $g$ is not convex (because $g$ can be seen as a function from $\\mathbb{R}^{nk + kp}$). Let consider $g_{1, nk + 1}$ (we put every coefficients of $W$ and $H$ to $0$ except the first of each matrix). \n",
    "\n",
    "For all $(w, h) \\in \\mathbb{R}^{2}$, we have $$g_{1, nk + 1}(w, h) = C(M) + \\frac{1}{2np}(M_{1, 1} - wh)^{2}$$ where $C(M)$ is a constant depending on the coefficients of M.\n",
    "\n",
    "Then, proving that $g_{1, nk + 1}$ is not convex is equivalent to prove that $$s : \\begin{array} \\left  \n",
    "\\mathbb{R} \\times \\mathbb{R} \\longrightarrow \\mathbb{R} \\\\\n",
    "(w, h) \\mapsto (1 - wh)^{2} \\end{array}$$ is not convex.\n",
    "\n",
    "We have $$\\left\\{\\begin{array} \n",
    "ss(-1, -1) = 0 \\\\\n",
    "s(1, 1) = 0 \\\\\n",
    "s \\left( \\frac{1}{2}(1, 1) + \\frac{1}{2}(-1, -1) \\right) = s(0, 0) = 1 \\end{array}\\right.$$\n",
    "\n",
    "Thus, $$s(0, 0) > \\frac{1}{2} s(1, 1) + \\frac{1}{2} s(-1, -1)$$\n",
    "The definition of convexity is not satisfied. $s$ is not convex, so $g_{1, nk+1}$ is not.\n",
    "\n",
    "<p style=\"text-align:center\";><span style=\"border:1px solid black;padding:1%\">Thus, the objective function $g$ is not convex.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let compute the gradient of $g$</b>. We have, for all $W, u \\in \\mathbb{R}^{n \\times k}$ and $H, v \\in \\mathbb{R}^{k \\times p}$:\n",
    "\n",
    "$$ \\begin{array} gg(W + u, H + v) & = \\frac{1}{2np} \\langle M - (W + u)(H + v), M - (W + u)(H + v) \\rangle_{F} \\\\\n",
    " & = \\frac{1}{2np} \\langle M - WH - Wv - uH - uv, M - WH - Wv - uH - uv \\rangle_{F} \\\\\n",
    " & = \\frac{1}{2np} \\langle M - WH, M - WH \\rangle_{F} - \\frac{1}{np} \\langle M - WH, Wv + uH \\rangle_{F} + o \\left((u, v)\\right) \\\\\n",
    " & = g(W, H) - \\frac{1}{np} \\langle M - WH, uH \\rangle_{F} - \\frac{1}{np} \\langle M - WH, Wv \\rangle_{F} + o \\left((u, v)\\right) \\\\\n",
    " & = g(W, H) - \\frac{1}{np} \\left( \\langle (M - WH)H^{T}, u \\rangle_{F} - \\langle W^{T}(M - WH), v \\rangle_{F} \\right) + o \\left((u, v)\\right) \\end{array} $$\n",
    "\n",
    "Then, $$\\boxed{\\nabla_{W}g(W, H) = - \\frac{1}{np} (M - WH)H^{T} \\,\\, and \\,\\, \\nabla_{H}g(W, H) = - \\frac{1}{np} W^{T} (M - WH)}$$\n",
    "\n",
    "This results could have been obtained directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>let prove that the gradient is not Lipschitz continuous</b>. \n",
    "\n",
    "In what follows we will consider the appropriate Frobenius's norms for all the spaces considered (e.g. for a tuple of matrices $(W,H)$ we will have the sum over all the coefficients of $W$ and $H$). \n",
    "\n",
    "Let $W \\in \\mathbb{R}^{n \\times k}, H, H' \\in \\mathbb{R}^{k \\times p}$ and $\\alpha > 0$. We have :\n",
    "\n",
    "$\\begin{array} \\nabla\\nabla_{W} g(W, \\alpha H) - \\nabla_{W} g(W, \\alpha H') & \n",
    "= - \\frac{1}{np} \\left[ \\alpha M H^{T} - \\alpha^{2} W HH^{T} - \\alpha M H'^{T} + \\alpha^{2} W H'H'^{T} \\right] \\\\\n",
    " & = - \\frac{1}{np} \\left[ \\alpha M \\left( H^{T} - H'^{T} \\right) + \\alpha^{2} W \\left( H'H'^{T} - HH^{T} \\right) \\right] \\\\\n",
    " & = - \\frac{\\alpha}{np} \\left[ M \\left( H^{T} - H'^{T} \\right) + \\alpha W \\left( H'H'^{T} - HH^{T} \\right) \\right] \\end{array}$\n",
    "\n",
    "Then, we have \n",
    "\n",
    "$\\left|\\left| \\nabla_{W} g(W, \\alpha H) - \\nabla_{W} g(W, \\alpha H') \\right|\\right| = \\frac{\\alpha}{np} \\left|\\left| M \\left( H^{T} - H'^{T} \\right) + \\alpha W \\left( H'H'^{T} - HH^{T} \\right) \\right|\\right|$\n",
    "\n",
    "and\n",
    "\n",
    "$\\left|\\left| (W, \\alpha H) - (W, \\alpha H') \\right|\\right| = \\alpha \\left|\\left| (0, H - H') \\right|\\right|$\n",
    "\n",
    "As $\\alpha > 0$, we can divide by $\\alpha$ and then compare the quantities : $$\\frac{1}{np} \\left|\\left| M \\left( H^{T} - H'^{T} \\right) + \\alpha W \\left( H'H'^{T} - HH^{T} \\right) \\right|\\right| \\,\\, and \\,\\, \\left|\\left| (0, H - H') \\right|\\right|$$\n",
    "\n",
    "For $u, v \\in \\mathbb{N}$, we denote by $E^{u, v}$ the matrix with size $u \\times v$ which has all its coefficients to $0$ except the first one that is equal to $1$.\n",
    "\n",
    "We take $H = E^{k, p}, H' = 2 E^{k, p}, W = E^{n, k}$. Thus, we have\n",
    "\n",
    "$$ \\frac{1}{np} \\left|\\left| M \\left( H^{T} - H'^{T} \\right) + \\alpha W \\left( H'H'^{T} - HH^{T} \\right) \\right|\\right| = \\frac{1}{np} \\left[ \\left( 3 \\alpha - M_{1, 1} \\right)^{2} + K(M) \\right]$$ \n",
    "\n",
    "where $K(M)$ is a constant because $M$ is fixed, and \n",
    "\n",
    "$$\\left|\\left| (0, H - H') \\right|\\right| = 1$$\n",
    "\n",
    "As $\\frac{1}{np} \\left[ \\left( 3 \\alpha - M_{1, 1} \\right)^{2} + K(M) \\right] \\underset{\\alpha \\rightarrow + \\infty}{\\longrightarrow} + \\infty$, we showed that :\n",
    "\n",
    "$$\\forall C \\geq 0, \\exists (x, y), \\left|\\left| \\nabla_{W} g(x) - \\nabla_{W} g(y) \\right|\\right| > C \\left|\\left| x - y \\right|\\right|$$\n",
    "\n",
    "<p style=\"text-align:center\";><span style=\"border:1px solid black;padding:1%\">Finally, $\\nabla_{W} g$ is not Lipschitz continuous, that is to say the gradient of $g$ is not Lipschitz continuous.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 - Find $W$ when $H_{0}$ is fixed </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.1 </h3>\n",
    "\n",
    "We initialize as follows. What is the advantage of this choice? What would be other possibilities for the initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "W0, S, H0 = svds(mini_db.astype(float), k)\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "H0 = np.maximum(0, (H0.T * np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this choice (i.e. using the Singular Value Decomposition) is that, when $M$ is a square matrix that satisfies $Sp \\left( M \\right) \\subset \\mathbb{R}_{+}^{*}$, this initialization is a decomposition close to the minimum of the objective function (and the exact minimum if $n = k = p$).\n",
    "\n",
    "For the initialization, we could take $W_{0} = 0$ and $H_{0} = 0$ but this decomposition is not likely to be as close to the minimum as the above decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first part, we would like to solve the simpler problem:\n",
    "\n",
    "$$g \\left( W \\right) = \\frac{1}{2np} \\left|\\left| M - WH_{0} \\right|\\right|_{F}^{2}$$\n",
    "\n",
    "$$W_{1} \\in \\arg \\min_{W \\geq 0} g \\left( W \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.2 </h3>\n",
    "\n",
    "Is the objective function $g$ convex? Calculate its gradient. We will admit that the gradient of $g$ is Lipschitz continuous with constant $L_{0} = \\frac{1}{np}\\|(H_{0})^{T}H_{0}\\|_{F}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g$ is a sum of convex quadratic functions. \n",
    "<p style=\"text-align:center\";><span style=\"border:1px solid black;padding:1%\">Thus, $g$ is convex.</span></p>\n",
    "\n",
    "Besides, we have $$\\boxed{\\nabla_{W} g(W) = - \\frac{1}{np} (M - WH_{0})H_{0}^{T}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.3 </h3>\n",
    "\n",
    "Write a function to compute $g(W)$ and another to compute $\\nabla g(W)$.\n",
    "You can check your computations using the function <tt>scipy.optimize.check_grad</tt> (as <tt>check_grad</tt> cannot deal with matrix variable, you may need to vectorize your variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unvectorize_M(W_H, M = mini_db):\n",
    "    # number of elements in W_H is (n+p)*k where M is of size n x m\n",
    "    # W has the nk first elements\n",
    "    # H has the kp last elements\n",
    "    n, p = M.shape\n",
    "    k = W_H.shape[0] // (n + p)\n",
    "    W = W_H[:n * k].reshape((n, k))\n",
    "    H = W_H[n * k:].reshape((k, p))\n",
    "    return W, H\n",
    "\n",
    "def vectorize(W, H):\n",
    "    return np.concatenate((W.ravel(), H.ravel()))\n",
    "\n",
    "def compute_obj_function(W_H, M = mini_db):\n",
    "    W_unvec, H_unvec = unvectorize_M(W_H, M)    \n",
    "    return (2*M.shape[0]*M.shape[1])**(-1) * np.linalg.norm(M - W_unvec @ H_unvec)**2\n",
    "\n",
    "def compute_grad_obj_function(W_H, M = mini_db):\n",
    "    W_unvec, H_unvec = unvectorize_M(W_H, M)\n",
    "    n, p = M.shape\n",
    "    grad_W = (- n*p)**(-1) * (M - W_unvec @ H_unvec) @ H_unvec.T\n",
    "    grad_H = (- n*p)**(-1) * W_unvec.T @ (M - W_unvec @ H_unvec)\n",
    "    return vectorize(grad_W, grad_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007838044905674255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_grad(compute_obj_function, compute_grad_obj_function, vectorize(W0,H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.4 </h3>\n",
    "\n",
    "Let us define the function:\n",
    "$$ \\iota_{\\mathbb{R}_{+}} : \\mathbb{R} \\rightarrow \\mathbb{R}\\cup\\{+\\infty\\}$$\n",
    "$$ x \\rightarrow \\left\\{ \\begin{matrix} 0 \\,\\,\\,\\, si \\,\\,\\,\\, x \\geq 0 \\\\ +\\infty \\,\\,\\,\\, si \\,\\,\\,\\, x < 0 \\end{matrix} \\right. $$ \n",
    "\n",
    "Show that for all $\\gamma > 0$, $prox_{\\gamma \\iota_{\\mathbb{R_{+}}}}$ is the projection onto $\\mathbb{R}_{+}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\gamma > 0, x \\in \\mathbb{R}$. Using the definition, we have :\n",
    "\n",
    "$prox_{\\gamma \\iota_{\\mathbb{R}_{+}}}(x) = arg\\displaystyle\\min_{y \\in \\mathbb{R}} \\left( \\gamma\\iota_{\\mathbb{R}_{+}}(y) + \\frac{|y - x|^{2}}{2} \\right)$\n",
    "\n",
    "$prox_{\\gamma \\iota_{\\mathbb{R}_{+}}}(x) = arg\\displaystyle\\min_{y \\in \\mathbb{R}_{+}} \\left( \\gamma\\iota_{\\mathbb{R}_{+}}(y) + \\frac{|y - x|^{2}}{2} \\right)$ because $\\gamma\\iota_{\\mathbb{R}_{+}}(y) = + \\infty$ if $y < 0$.\n",
    "\n",
    "$prox_{\\gamma \\iota_{\\mathbb{R}_{+}}}(x) = arg\\displaystyle\\min_{y \\in \\mathbb{R}_{+}} \\left( |y - x| \\right)$ because $\\iota_{\\mathbb{R}_{+}}(y) = 0$ for $y \\geq 0$.\n",
    "\n",
    "$prox_{\\gamma \\iota_{\\mathbb{R}_{+}}}(x) = \\left\\{ \\begin{array} xx \\,\\, si \\,\\, x \\geq 0 \\\\\n",
    "0 \\,\\, si \\,\\, x < 0 \\end{array} \\right.$\n",
    "\n",
    "This is the point of $\\mathbb{R}_{+}$ closest to $x$, i.e. $proj_{\\mathbb{R}_{+}}(x)$. Thus,\n",
    "\n",
    "$$\\boxed{\\forall \\gamma > 0, prox_{\\gamma \\iota_{\\mathbb{R}_{+}}} = proj_{\\mathbb{R}_{+}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.5 </h3>\n",
    "\n",
    "Code a function <tt>projected\\_gradient\\_method(val\\_g, grad\\_g, W0, gamma, N)</tt> that minimizes a function $g$ subject to nonnegativity constraints by the projected gradient method with a constant step size $\\gamma$, starting from $W_{0}$ and stopping after $N$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method(val_g, grad_g, W0, gamma, N):\n",
    "    x = W0\n",
    "    for k in range(N):\n",
    "        x = np.maximum(0, x - gamma * unvectorize_M(grad_g(vectorize(x, H0)))[0])\n",
    "    return val_g(vectorize(x, H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 3.6 </h3>\n",
    "\n",
    "Use the function to minimize $g$ with $N = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projected gradient method returns : 442.2878543960478\n"
     ]
    }
   ],
   "source": [
    "gamma = 1 / (np.linalg.norm(H0.T.dot(H0), 'fro') / (n * p))\n",
    "\n",
    "t1_proj = time.time()\n",
    "min_g_proj = projected_gradient_method(compute_obj_function, compute_grad_obj_function, W0, gamma, 100)\n",
    "t2_proj = time.time()\n",
    "len_proj = t2_proj - t1_proj\n",
    "\n",
    "print('The projected gradient method returns: ' + str(min_g_proj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4 - Algorithmic refinement for the problem with $H_{0}$ fixed </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 4.1 </h3>\n",
    "\n",
    "Implement a line search to the projected gradient method, in order to free ourselves from the need of a known Lipschitz constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial value of g is : 456.31969468823496\n"
     ]
    }
   ],
   "source": [
    "g_init = compute_obj_function(vectorize(W0, H0))\n",
    "print('The initial value of g is: ' + str(g_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_line_search(val_g, grad_g, W, gamma):\n",
    "    a, b = 0.5, 2 * gamma\n",
    "    i = 0\n",
    "    gamma2 = b * (a**i)\n",
    "    x = W - gamma2 * unvectorize_M(grad_g(vectorize(W, H0)))[0]\n",
    "    while (val_g(vectorize(x, H0)) > \n",
    "           val_g(vectorize(W, H0)) - (gamma2/2) * np.linalg.norm(unvectorize_M(grad_g(vectorize(W, H0)))[0])**2):\n",
    "        gamma2 *= a\n",
    "        x = W - gamma2 * unvectorize_M(grad_g(vectorize(W, H0)))[0]\n",
    "    return gamma2\n",
    "    \n",
    "def projected_gradient_line_search_method(val_g, grad_g, W0, N):\n",
    "    gamma = 1\n",
    "    x = W0\n",
    "    for k in range(N):\n",
    "        gamma = taylor_line_search(val_g, grad_g, x, gamma)\n",
    "        x = np.maximum(0, x - gamma * unvectorize_M(grad_g(vectorize(x, H0)))[0])\n",
    "    return val_g(vectorize(x, H0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projected gradient method with line search returns : 445.4267145889815\n"
     ]
    }
   ],
   "source": [
    "t1_line = time.time()\n",
    "min_g_line = projected_gradient_line_search_method(compute_obj_function, compute_grad_obj_function, W0, 100)\n",
    "t2_line = time.time()\n",
    "len_line = t2_line - t1_line\n",
    "\n",
    "print('The projected gradient method with line search returns: ' + str(min_g_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 4.2 </h3>\n",
    "\n",
    "Compare the performance of both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialization gives us : g = 456.31969468823496\n",
      "\n",
      "With the projected gradient method, we have : g = 442.2878543960478, obtained in : 0.18251252174377441 s. \n",
      "\n",
      "With the line search gradient method, we have : g = 445.4267145889815, obtained in : 1.1948065757751465 s. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The initialization gives us: g = ' + str(g_init) + '\\n\\n' +\n",
    "      'With the projected gradient method, we have: g = ' + str(min_g_proj) + ', obtained in : ' + str(len_proj) + ' s. \\n\\n' +\n",
    "      'With the line search gradient method, we have: g = ' + str(min_g_line) + ', obtained in : ' + str(len_line) + ' s. \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5 - Resolution of the full problem </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5.1 </h3>\n",
    "\n",
    "Solve Problem (1) by the projected gradient method with line search for $N = 1000$ iterations. What does the algorithm return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_taylor_line_search(val_g, grad_g, W_H, gamma):\n",
    "    a, b = 0.5, 2 * gamma\n",
    "    i = 0\n",
    "    gamma2 = b * (a**i)\n",
    "    x = W_H - gamma2 * grad_g(W_H)\n",
    "    while (val_g(x) > \n",
    "           val_g(W_H) - (gamma2/2) * np.linalg.norm(grad_g(W_H))**2):\n",
    "        gamma2 *= a\n",
    "        x = W_H - gamma2 * grad_g(W_H)\n",
    "    return gamma2\n",
    "\n",
    "def full_projected_gradient_method(val_g, grad_g, W0_H0, N):\n",
    "    gamma = 1\n",
    "    x_y = W0_H0\n",
    "    \n",
    "    for k in range(N) :\n",
    "        gamma = full_taylor_line_search(val_g, grad_g, x_y, gamma)\n",
    "        x_y = np.maximum(0, x_y - gamma * grad_g(x_y))\n",
    "    \n",
    "    W, H = unvectorize_M(x_y)\n",
    "    return W, H, val_g(x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projected gradient method with line search applied to both variables returns : 358.9223041984623\n"
     ]
    }
   ],
   "source": [
    "W_f, H_f, fproj = full_projected_gradient_method(compute_obj_function, compute_grad_obj_function, vectorize(W0, H0), 1000)\n",
    "\n",
    "print('The projected gradient method with line search applied to both variables returns: ' + str(fproj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the cell above, the result is better than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5.2 </h3>\n",
    "\n",
    "When $W$ (resp. $H$) is fixed, the problem is easier to solve. The method of alternate minimizations uses this fact and consists in the following method:\n",
    "\n",
    "> <b>for</b> $t \\geq 1$ <b>do</b> <br/>\n",
    "$\\,\\,\\,\\, W_{t} \\leftarrow \\arg \\min_{W} \\frac{1}{2np} \\left|\\left| M - WH_{t-1} \\right|\\right|_{F}^{2}$ <br/>\n",
    "$\\,\\,\\,\\, H_{t} \\leftarrow \\arg \\min_{H} \\frac{1}{2np} \\left|\\left| M - W_{t}H \\right|\\right|_{F}^{2}$<br/>\n",
    "<b>end for</b>\n",
    "\n",
    "Show that the value of the objective function is decreasing at each iteration. Deduce from this that the value converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: $g$ is defined by $g : (W, H) \\mapsto \\frac{1}{2np} \\left|\\left| M - WH \\right|\\right|_{F}^{2}$.\n",
    "\n",
    "Let $t \\geq 1$ and show that $g(W_{t}, H_{t}) \\leq g(W_{t-1}, H_{t-1})$. \n",
    "\n",
    "By construction, for all $W \\in \\mathbb{R}^{n \\times k}$, we have $g(W_{t}, H_{t-1}) \\leq g(W, H_{t-1})$. In particular, we have $g(W_{t}, H_{t-1}) \\leq g(W_{t-1}, H_{t-1})$.\n",
    "\n",
    "Besides, for all $H \\in \\mathbb{R}^{k \\times p}$, we have $g(W_{t}, H_{t}) \\leq g(W_{t}, H)$. In particular, we have $g(W_{t}, H_{t}) \\leq g(W_{t}, H_{t-1})$.\n",
    "\n",
    "Then, by transitivity, we have: $$ g(W_{t}, H_{t}) \\leq g(W_{t-1}, H_{t-1}) $$\n",
    "\n",
    "<p style=\"text-align:center\";><span style=\"border:1px solid black;padding:1%\">$\\left( g(W_{t}, H_{t}) \\right)_{t \\geq 1}$ is a decreasing suite, lower-bounded by $0$. Then, the suite converges.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5.3 </h3>\n",
    "\n",
    "Code the alternate minimizations method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_line_search_W(val_g, grad_g, W, H0, gamma):\n",
    "    a, b = 0.5, 2 * gamma\n",
    "    i = 0\n",
    "    gamma2 = b * (a**i)\n",
    "    x = W - gamma2 * unvectorize_M(grad_g(vectorize(W, H0)))[0]\n",
    "    while (val_g(vectorize(x, H0)) > \n",
    "           val_g(vectorize(W, H0)) - (gamma2/2) * np.linalg.norm(unvectorize_M(grad_g(vectorize(W, H0)))[0])**2):\n",
    "        gamma2 *= a\n",
    "        x = W - gamma2 * unvectorize_M(grad_g(vectorize(W, H0)))[0]\n",
    "    return gamma2\n",
    "\n",
    "def projected_gradient_method_linesearch_W(val_g, grad_g, W0, H0, N):\n",
    "    gamma = 1\n",
    "    x = W0\n",
    "    for k in range(N):\n",
    "        gamma = taylor_line_search_W(val_g, grad_g, x, H0, gamma)\n",
    "        x = np.maximum(0, x - gamma * unvectorize_M(grad_g(vectorize(x, H0)))[0])\n",
    "    return x\n",
    "\n",
    "def taylor_line_search_H(val_g, grad_g, W0, H, gamma):\n",
    "    a, b = 0.5, 2 * gamma\n",
    "    i = 0\n",
    "    gamma2 = b * (a**i)\n",
    "    y = H - gamma2 * unvectorize_M(grad_g(vectorize(W0, H)))[1]\n",
    "    while (val_g(vectorize(W0, y)) > \n",
    "           val_g(vectorize(W0, H)) - (gamma2/2) * np.linalg.norm(unvectorize_M(grad_g(vectorize(W0, H)))[0])**2):\n",
    "        gamma2 *= a\n",
    "        y = H - gamma2 * unvectorize_M(grad_g(vectorize(W0, H)))[1]\n",
    "    return gamma2\n",
    "\n",
    "def projected_gradient_method_linesearch_H(val_g, grad_g, W0, H0, N):\n",
    "    gamma = 1\n",
    "    y = H0\n",
    "    for k in range(N):\n",
    "        gamma = taylor_line_search_H(val_g, grad_g, W0, y, gamma)\n",
    "        y = np.maximum(0, y - gamma * unvectorize_M(grad_g(vectorize(W0, y)))[1])\n",
    "    return y\n",
    "\n",
    "def projected_alternate_minimization_method(val_g, grad_g, W0, H0, N):\n",
    "    x = W0\n",
    "    y = H0\n",
    "    for k in range(N):\n",
    "        x = projected_gradient_method_linesearch_W(val_g, grad_g, x, y, N//10)\n",
    "        y = projected_gradient_method_linesearch_H(val_g, grad_g, x, y, N//10)\n",
    "    return x, y, val_g(vectorize(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alternate minimizations method returns : 358.939133356636\n"
     ]
    }
   ],
   "source": [
    "W_a, H_a, alternative_min = projected_alternate_minimization_method(compute_obj_function, compute_grad_obj_function, W0, H0, 100)\n",
    "\n",
    "print('The alternate minimizations method returns: ' + str(alternative_min)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5.4 </h3>\n",
    "\n",
    "Compare projected gradient and alternate minimizations methods. Are the solutions the same? Is the objective value the same? How do the computing times compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "t1_f = time.time()\n",
    "W_f, H_f, min_g_fproj = full_projected_gradient_method(compute_obj_function, compute_grad_obj_function, vectorize(W0, H0), N)\n",
    "t2_f = time.time()\n",
    "len_f = t2_f - t1_f\n",
    "\n",
    "t1_a = time.time()\n",
    "W_a, H_a, min_g_alternative = projected_alternate_minimization_method(compute_obj_function, compute_grad_obj_function, W0, H0, N)\n",
    "t2_a = time.time()\n",
    "len_a = t2_a - t1_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the projected method, we get : \n",
      "\n",
      "W = [[ 0.         71.87194646]\n",
      " [ 0.         78.95671828]\n",
      " [ 0.         73.78427977]\n",
      " [58.61889631 44.95150838]\n",
      " [42.82422019 54.7030421 ]\n",
      " [19.70616719 66.01665216]\n",
      " [15.67926382 65.83761374]\n",
      " [49.26104602 45.4162348 ]\n",
      " [69.34129776 36.40236423]\n",
      " [33.16588484 55.23490189]]\n",
      "\n",
      "H = [[0.38022727 0.32407219 0.21108828 ... 0.26975117 0.14660154 0.15375377]\n",
      " [0.62086486 0.68036239 0.69322862 ... 0.70915623 0.71735043 0.75100897]]\n",
      "\n",
      "g(W, H) = 363.5666859963321\n",
      "\n",
      "in 0.556485652923584 seconds\n",
      "\n",
      "\n",
      "With the alternate minimizations method, we get : \n",
      "\n",
      "W = [[  8.99411112  69.99680342]\n",
      " [  8.60837769  77.61389637]\n",
      " [  0.          76.51990801]\n",
      " [ 91.45289359  33.27808448]\n",
      " [ 78.98961077  40.74758491]\n",
      " [ 29.81289594  64.6332918 ]\n",
      " [ 31.26695766  61.36509863]\n",
      " [ 89.45646221  29.38187546]\n",
      " [113.30233658  19.25822191]\n",
      " [ 70.59605747  40.061465  ]]\n",
      "\n",
      "H = [[0.33001953 0.30930524 0.24844045 ... 0.2449774  0.18905016 0.18328004]\n",
      " [0.59940083 0.65791425 0.67107203 ... 0.72638249 0.72338144 0.77240311]]\n",
      "\n",
      "g(W, H) = 358.939133356636\n",
      "\n",
      "in 6.819776773452759 seconds\n"
     ]
    }
   ],
   "source": [
    "print('With the projected method, we get: ' + '\\n\\n' +\n",
    "      'W = ' + str(W_f) + '\\n\\n' +\n",
    "      'H = ' + str(H_f) + '\\n\\n' + \n",
    "      'g(W, H) = ' + str(min_g_fproj) + '\\n\\n' +\n",
    "      'in ' + str(len_f) + ' seconds' + '\\n\\n\\n' +\n",
    "      'With the alternate minimizations method, we get: ' + '\\n\\n' +\n",
    "      'W = ' + str(W_a) + '\\n\\n' +\n",
    "      'H = ' + str(H_a) + '\\n\\n' + \n",
    "      'g(W, H) = ' + str(min_g_alternative) + '\\n\\n' +\n",
    "      'in ' + str(len_a) + ' seconds'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that the alternate minimizations method gives a better minimum than the projection gradient method, but it is longer in terms of complexity (6.8s vs 0.6s). Besides, it shows that the solution obtained with the alternate minimizations method is more precise than the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Question 5.5 </h3>\n",
    "\n",
    "What stopping criterion could be used for the algorithms instead of just the number of iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a number of iterations, we could stop iterating when the norm of the difference between two successive values is less than a threshold that is fixed by the user (e.g. $\\epsilon = 10^{-6}$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
